\documentclass[11pt]{article}

\usepackage[margin=1.0in]{geometry}
\usepackage{fullpage}
\usepackage{url}
\usepackage{paralist}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage[small]{caption}
\usepackage{subfig}
\usepackage{multirow}
\usepackage{color}


%%% TO DO
%-- Intro : basically no language, change first paragraph (Aaron)
%    -- Mention bolt
%    -- Confidence and uncertainty
%-- Prior Work on nudging (Everyone)
%-- Methodology
%    -- Info about Kinect (Rob)
%    -- Object Detection and Tracking (Lauren)
%    -- No gathering data? (James)
%    -- arm manipulation (Rob)
%    -- Active Learning
%    -- multiclass clustering in KNN, adding new classes is easy (James)
%    -- more thoroughly explain nudging selection process (James)
%    -- reorder sentences indata collection p1 (James)
%-- Evaluation
%    -- paragraph explaining scenario
%    -- training set size (James)
%-- Future work section
%    -- Expand on BOLT
%-- Conclusion



% The xxx tag is intended to denote urgent text that needs addressing.
% The meh tag is intended to highlight text that needs some loving or which
% we're not sure should make primetime
\newcommand{\xxx}[1]{{\bf \color{red} #1}}
\newcommand{\meh}[1]{{\bf \color{blue} #1}}
\newcommand\T{\rule{0pt}{3ex}}
\newcommand\B{\rule[-1.2ex]{0pt}{0pt}}

\title{Reducing Uncertainty and Acquiring Knowledge Through Interaction}
\author{Rob Goeddel \and Lauren Hinkle \and James Kirk \and Aaron Mininger}
\date{}

\begin{document}
\maketitle


\begin{abstract}
Interaction between robots and humans is a rapidly growing area of research.
Of particular interest is the desire to interact with robots or agents using
natural language. This gives humans a more natural and effective way of teaching,
commanding, and interacting with robotic agents or computers. Our work is
designed to process images and provide descriptions of objects that can then be
used in interactions with humans. We use images from an RGB-D camera in order to
classify properties of objects including size, shape, and color. These
descriptions are then fed into a higher-level agent which handles the human
interaction. We specifically use a simple I-Spy game to allow users to refer to
objects using descriptions in natural language.
\end{abstract}


\section{Introduction}
%\xxx{Problem description and motivation. Why do you want to solve this
%    problem? What's the impact if you can solve this problem?}
Language is a powerful tool that is just coming into its own as the human
interface for various systems. Apple's Siri is an excellent example of this,
making interaction with one's phone intuitive, easy, and efficient. Robotic
systems in AI can also benefit from greater understanding of language. Such
knowledge could allow a human to command or teach a robot agent in a more natural
way. Take, for example, a robotic arm system with knowledge of only several nouns
and actions: cup, sink, grab/move object, and turning on/off the sink, and the
spatial concept of in. Through language, the robot could then be taught the new
action of ``filling a cup'', which means to put the cup in the sink and turn on
the sink. Even though the robot has never seen anyone fill a cup before, now it
should be able to do so itself. Furthermore, the agent could generalize the
concept to new items, like a bowl. This is a powerful way for an agent to learn
from a human, without requiring expert instructors.

Our long-term goal is to leverage machine learning techniques in order to train
a system to know a small vocabulary of nouns, adjectives, and actions. As a proof
of concept, we develop a system that can play the children's game ``I spy'' using
a robotic arm for pointing at and interacting with objects and a Microsoft Kinect
which acts as the ``eyes'' of the system, giving RGB-D data about the scene for
classification. To play I Spy, a collection of objects is placed in the view
of the camera and a person gives a description of one of the objects. The agent
then determines which object is most likely to match the description, and points
to it using the robot arm. If the agent is highly uncertain about the objects, it
interacts with them using the arm in order to gain further information. If the
agent guesses incorrectly, the human then shows them the correct answer, and
the agent adjusts its beliefs about object descriptions to reflect its new
knowledge.

This work is being incorporated in a larger project which includes teaching the
agent new concepts and actions as described above. Such a project relies on having
symbolic information provided by our work through object segmentation and
classification. The low level features provided by our system can be analyzed and
combined in order to build new concepts such as object categories (like blocks
and balls) or object properties (all bananas are yellow). Such reasoning and
concept generalization is essential for agents that interact in novel situations.
These capabilities could be used on service tasks or with non-robotic systems
where a verbal interface offers improvements in efficiency, safety, etc.

The challenges involved in playing I Spy include object detection, feature
extraction, classification, robot arm manipulation, and active learning. We
discuss these further in Section 3.

\section{Related Work}
%\xxx{What are existing methods? What are the state-of-the-art methods for this
%    problem? How is your approach different from the related work?}

Object classification is a problem that appears throughout a variety of
applications, ranging from law enforcement to manufacturing. While historic
efforts have focused on the recognition problem, that is, finding a known object
in a scene, more recent work has emphasized the more general problem of
classifying objects into generic groups by type~\cite{huber2004parts}. For example,
we might be interested in identifying vehicles as bikes, motorcycles, trucks,
cars, etc. Work by~\cite{nilsback2006visual} and~\cite{gehler2009feature}, among
others, has shown that powerful classifiers may be constructed through boosting
or similar simple feature-specific classifiers such as color, shape, etc.

Recent popularization of sensors that combine both image and depth information
like the Microsoft Kinect inexpensively provide richer sensory information. The
addition of depth can help disambiguate situations that would be very difficult
with a normal camera image alone. Depth can make the determination of an objectâ€™s
underlying geometry much easier, leading to better identification
~\cite{marton2010hierarchical}.
Previous work has successfully incorporated such cameras (often called RGB-D
cameras) with object recognition in indoors domains featuring common household
objects~\cite{marton2010hierarchical, lai2011sparse}.

A different approach to this problem is learning words that describe different
areas of the feature space, which can then be combined to describe classes of
objects. Research in this area focuses on learning to map physically observable
qualities to simple words with the intent of using these simple descriptors to
learn more complex nouns and relationships. Of particular popularity is an
approach focused on learning colors and shapes of small objects using sensory
data~\cite{zambuto2010visually, roy2002learning}. Additional approaches focus on
learning entities in the environment through interaction with
them~\cite{gold2009robotic}. These approaches all focus on interaction with
physical objects to discover qualities about them.

Although the types of words learned in these works is similar, the approaches and
end-goals vary. Semantic clustering~\cite{zambuto2010visually}, decision trees
~\cite{gold2009robotic}, and bag of word models~\cite{roy2002learning} are all
used in the learning algorithms. This prior work focuses mostly on visual
classification and the corresponding vocabulary.

Though our work does not extend dramatically on this concept, our long term
trajectory focuses more on the complex action space, previously less explored.
This project lays the groundwork for developing an action-space, both for
learning said actions as well as learning objects upon which to act.
Additionally, we explore a novel way of extracting additional information from
objects. We are unaware of previous work that uses interaction with objects to
see them from new angles and thus obtain additional information to make guesses
about object descriptions more confident.


\section{Methodology}
%\xxx{How are you going to solve this problem? Why should the proposed method
%    work? Provide technical details of your approach if you are proposing a
%    novel method.}

\subsection{Object Detection}
\xxx{This section is definitely not done yet}
In each image the ground plane is removed, using RANSAC to discover the dominant
plane in the image, isolating most of the objects in the scene. Further
segmentation is performed to try to discover definitive object boundaries. From
these segmentations we extract a feature vector for each object.

For object identification we will do a similar process to segment each object in
the camera's image. That object's position in 3D space will be determined using
both the pixel coordinate and the depth value.

\subsection{Feature Extraction}

Once an object was segmented we extracted relevant features in order to classify
color, size, and shape. We considered objects that were of a single color, therefore
average red, green, blue, hue, saturation, and value levels across the pixels were
used as a six-dimensional feature vector. For size we calcuated the diagonal of the 3D
axis-aligned bounding box and used that as one of our features. The second feature was
the average distance of the points from the centroid. Having the point-cloud data from
the Kinect allowed us to compute those features more accurately in three-dimensions
and saved us from having to do more complicated depth estimates.

Correctly classifying shapes required much more complicated features.
During this work we explored several options for extracting features from
objects before settling on the method described in this paper. We experimented
with ORB, a rotation invariant feature extractor similar to SIFT and SURF
~\cite{rublee2011orb}. Additionally we tested several feature detectors from
OpenCV, including their square detector~\cite{opencv_library}. We also explored
using RANSAC to discover simple shape types (such as planes, curves, and spheres),
in order to determine the number of faces and edges each object has. This work
was based off the RANSAC algorithms of Schnabel et
al~\cite{schnabel2007efficient}. Although these and other feature detectors are
frequently used in object recognition and learning tasks, we ultimately chose to
use our own PCA-based method which resulted in more accurate guesses from the
agent than the other methods. First we
projected the point-cloud back onto the image plane. Next we transformed the image
into a canonical representation. To do this we applied Principal Component Analysis to
find the axis the points were most spread out along, and rotated the image so that
axis was horizontal. An axis-aligned bounding box was found for the image and its
heigh and width were scaled to unit lenghts. To extract features we took evenly-spaced
points from the top and bottom edges of the box and calculated the vertical distance
from each point to the image. This process is illustrated by [FIGURE]. We used seven
sample points along the top and bottom; experimental evaluation showed no signficant
improvement with additional samples. In addition, we calculated the ratio of the width
and height before scaling to get an idea of how elongated the shape was.
\xxx{Can we include some equations here in addition to the qualitative description
of the process?}

\subsection{Classification}
Initially we used SVM classification to validate our method, but we have since
moved to a K-nearest neighbor approach.  With the K-nearest neighbor algorithm
it is easy to add another example to the training, without recreating the
entire training model as was done with SVM classification.  Additionally we no
longer rely on third party software, namely liblinear ~\cite{LIBLINEAR} to do
classification.  Using our implementation of a K-nearest neighbor
classification we also reported a confidence metric for each label based on the
percentage of neighbors with a given label.

\subsection{Confidence}
One of the difficulties with reasoning about real objects is that the world is
partially observable. Objects can have a dramatically different appearance at
different orientations.  Naturally when given such an object, a human may move
their head around to gain more information and confidence as to what the object is.
Our system has a fixed vision angle, but it is possible for it to
manipulate objects to gain information on objects whose appearance has high
orientation dependency.  For this purpose a confidence threshold system was
developed to learn what objects may have high uncertainty, and therefor would
be good candidate for manipulation.  When the system is queried for an object
that it cannot currently see, it should manipulate an uncertain object with the
highest probability of actually being the queried object.

All attributes are given the same default confidence threshold.  When an object
is correctly labeled and verified by the user, the thresholds for those labels
is decreased.  When an object manipulation correctly reveals that it was
previously mislabeled the associated label thresholds are increased.  To
determine the likelihood of a mislabel, the confidence of a label, calculated
from the classification system, is compared to the threshold for that shape.
If the confidence exceeds the threshold then it is reported as unlikely to be a
mislabel.  For example in our system the rectangle label is commonly misplaced
on objects presented head on, so the learned threshold confidence for rectangle
is very high.

\subsection{Robot Arm Manipulation}

\subsection{Active Learning}


\xxx{equations}
\section{Experimental Results}

\subsection{Data Collection and Training}
We acquired a large variety of children's foam blocks in many shapes, sizes,
and colors to act as our objects for classification. A small subset of the
blocks can be seen in Figure~\ref{fig:blocks}. The foam of the blocks is an
ideal material to work with as it. Additionally we collected
more familiar objects to test our classifier on, demonstrating the ability of
the system to abstract general shapes to real world objects. Some of these
objects can be seen in Figure \meh{figure}.
\begin{inparaenum}[(1)]
\item has a smooth, matte finish,
\item comes in a variety of distinct colors, and
\item is deformable enough to be grabbed by the arm but still retain its
shape.
\end{inparaenum}


Our system, seen in figure \meh{screenshot}, segments objects found in the
predefined play area and labels them with the most confident shape, size, and
color.  Additionally a user can train new shapes, colors, and sizes by simply
clicking on that image and supplying the relevant label.  This is sufficient
for creating a large number of training samples, preferably at different
positions and orientations.  This method is not efficient for generating a
large number of training samples of many different objects, so we collected a
training dataset of footage of the foam blocks at numerous positions and angles
throughout the play area which were then segmented and labeled with relevant
descriptors.


\begin{figure}[h!]
\centering
\subfloat[Foam Blocks] {
    \includegraphics[height=1.6in]{figures/blocks.png}
    \label{fig:blocks}
}
\subfloat[Segmented Images] {
    \includegraphics[height=1.6in]{figures/blue_objs.png}
    \label{fig:images}
}
\subfloat[Real-world test data] {
    \includegraphics[height=1.6in]{figures/colorplot_noorange.eps}
    \label{fig:colordata}
}
\caption{Real world data is extracted from toy foam blocks. The blocks
    themselves can be seen in \subref{fig:blocks}. Using a segmentation
    algorithm, we extract the blocks from the Kinect data \subref{fig:images}
    and generate feature vectors for each object.
    Finally, we hand label the data for features such as color and shape for
    classification training. In \subref{fig:colordata}, we see the actual
    distribution of our training samples in the hue and saturation
    dimensions.}
\label{fig:objects}
\end{figure}





Testing our system requires direct interaction and presentation of new objects
or objects at new angles.  Additional testing can be done by training the
system with new shapes and colors and observing how quickly and effectively it
can learn and apply the new information.  Qualitatively it is easy to see
whether the system is performing well.  However to do a less subjective,
quantitative evaluation of our methodology, we used the leave-one-out-cross-validation(LOOCV) method on our collected training data.  This testing method mirrors the
scenario where a novel object is presented to this system.

The results in Table~\ref{tbl:testresults} \xxx{todo} show LOOCV percentages
for the shape, color and size.  Unsurprisingly it has more difficulty with shape
, which varies greatly by angle and position, and size, which is a more
subjective attribute. It has particular problems with shapes that are presented
head on, which tend to resemble rectangles.  This is the primary motivation for
developing a confidence threshold learning mechanism, which allows the system
to determine which labels are inherently uninformative.


% orange 87.10
\begin{table}
\centering
\begin{tabular}{ | l | l | l | l | l | l |}
    \hline
    red &  yellow & green & blue & purple \T \B \\ \hline
    96.43\%  & 92.00\% & 97.37 \% & 100.00\% & 88.57\% \B \T \\ \hline
\end{tabular}
\caption{This shows the result of OVA classification for five color types
    across 160 objects. Each test involved training on 80\% of the points,
           with 20\% left out for testing.}
\label{tbl:testresults}
\end{table}



\section{Conclusion}
%\xxx{Summary of your progress and your final expected goal (what do you expect
%    to achieve or demonstrate for the final project?)}


\bibliographystyle{plain}
\bibliography{literature}

\end{document}

% LocalWords:  multiclass
